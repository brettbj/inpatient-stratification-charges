{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\"\n",
    "\n",
    "import matplotlib \n",
    "matplotlib.use('agg')\n",
    "\n",
    "import platform; print(platform.platform())\n",
    "import sys; print(\"Python\", sys.version)\n",
    "import numpy; print(\"NumPy\", numpy.__version__)\n",
    "import scipy; print(\"SciPy\", scipy.__version__)\n",
    "import sklearn; print(\"Scikit-Learn\", sklearn.__version__)\n",
    "import seaborn as sns; print(\"Seaborn\", sns.__version__)\n",
    "\n",
    "import matplotlib.pyplot as plt;\n",
    "\n",
    "import sys\n",
    "import findspark\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "import numpy as np\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf; print(tf.__version__)\n",
    "import pyarrow as pa\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)\n",
    "\n",
    "fs = pa.hdfs.connect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = []\n",
    "base_file = \"./dashboard/Features_random_fold\"\n",
    "\n",
    "\n",
    "for k in range(8):\n",
    "    train_files.append(base_file + str(k) + \".parquet\")\n",
    "print(train_files)\n",
    "\n",
    "\n",
    "val_files = []\n",
    "val_files.append(base_file + \"8.parquet\")\n",
    "print(val_files)\n",
    "\n",
    "\n",
    "test_files = []\n",
    "test_files.append(base_file  + \"9.parquet\")\n",
    "print(test_files)\n",
    "\n",
    "\n",
    "from petastorm import make_reader\n",
    "from petastorm.tf_utils import tf_tensors, make_petastorm_dataset\n",
    "\n",
    "train_readers = []\n",
    "train_datasets = []\n",
    "\n",
    "val_readers = []\n",
    "val_datasets = []\n",
    "\n",
    "test_readers = []\n",
    "test_datasets = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train in train_files:\n",
    "    print('train: ', train)\n",
    "    r = make_reader(train, hdfs_driver='libhdfs', num_epochs=None, results_queue_size=1024, workers_count=8)\n",
    "    train_readers.append(r)\n",
    "    train_datasets.append((make_petastorm_dataset(r)\\\n",
    "                           .map(lambda x:((tf.reshape(x.EMBEDDING[-1000:], (100, 10)),\n",
    "                                           x.FEATURES),\n",
    "                                          tf.one_hot(tf.cast(x.readmitLabel, tf.uint8), 2))\n",
    "                               )\n",
    "                          )\n",
    "                         )\n",
    "    \n",
    "for val in val_files:\n",
    "    print('val: ', val)\n",
    "    r = make_reader(val, hdfs_driver='libhdfs', num_epochs=None, results_queue_size=1024, workers_count=8)\n",
    "    val_readers.append(r)\n",
    "    val_datasets.append((make_petastorm_dataset(r)\\\n",
    "                           .map(lambda x:((tf.reshape(x.EMBEDDING[-1000:], (100, 10)),\n",
    "                                           x.FEATURES),\n",
    "                                          tf.one_hot(tf.cast(x.readmitLabel, tf.uint8), 2))\n",
    "                               )\n",
    "                          )\n",
    "                         )\n",
    "    \n",
    "for test in test_files:\n",
    "    print('test: ', test)\n",
    "    r = make_reader(test, hdfs_driver='libhdfs', num_epochs=None, results_queue_size=1024, workers_count=8)\n",
    "    test_readers.append(r)\n",
    "    test_datasets.append((make_petastorm_dataset(r)\\\n",
    "                           .map(lambda x:((tf.reshape(x.EMBEDDING[-1000:], (100, 10)),\n",
    "                                           x.FEATURES),\n",
    "                                          tf.one_hot(tf.cast(x.readmitLabel, tf.uint8), 2))\n",
    "                               )\n",
    "                          )\n",
    "                         )\n",
    "    \n",
    "print(len(train_datasets), len(val_datasets), len(test_datasets))\n",
    "\n",
    "train_ds = tf.data.experimental.sample_from_datasets(train_datasets)\n",
    "val_ds = tf.data.experimental.sample_from_datasets(val_datasets)\n",
    "test_ds = tf.data.experimental.sample_from_datasets(test_datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test in test_files:\n",
    "    print('test: ', test)\n",
    "    r = make_reader(test, hdfs_driver='libhdfs', num_epochs=None, results_queue_size=1024, workers_count=8)\n",
    "    test_readers.append(r)\n",
    "    test_datasets.append((make_petastorm_dataset(r)\\\n",
    "                           .map(lambda x:((tf.reshape(x.EMBEDDING[-1000:], (100, 10)),\n",
    "                                           x.FEATURES),\n",
    "                                          tf.one_hot(tf.cast(x.readmitLabel, tf.uint8), 2))\n",
    "                               )\n",
    "                          )\n",
    "                         )\n",
    "test_ds = tf.data.experimental.sample_from_datasets(test_datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, CuDNNGRU, CuDNNLSTM, Dropout, Flatten\n",
    "from tensorflow.python.keras.layers.embeddings import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.utils import multi_gpu_model\n",
    "\n",
    "feat_shape = (1320)\n",
    "embed_shape = (100, 10)\n",
    "\n",
    "input_emb = Input(shape=(embed_shape[0], embed_shape[1]))\n",
    "lstm1 = CuDNNGRU(256, return_sequences=True)(input_emb)\n",
    "dropout = Dropout(0.01)(lstm1)\n",
    "lstm2 = CuDNNGRU(128, return_sequences=True)(dropout)\n",
    "dropout1 = Dropout(0.01)(lstm2)\n",
    "lstm3 = CuDNNGRU(64)(dropout1)\n",
    "\n",
    "input_feat = Input(shape=(feat_shape,))\n",
    "dense1 = Dense(256, activation='relu')(input_feat)\n",
    "dropout3 = Dropout(0.01)(dense1)\n",
    "dense2 = Dense(128, activation='relu')(dropout3)\n",
    "dense_flat = Flatten()(dense2)\n",
    "\n",
    "merged_vector = keras.layers.concatenate([dense_flat, lstm3], axis=-1)\n",
    "dense3 = Dense(128, activation='relu')(merged_vector)\n",
    "\n",
    "output = Dense(2, activation='softmax')(dense3)\n",
    "model = Model(inputs=[input_emb, input_feat],outputs=[output])\n",
    "\n",
    "model = multi_gpu_model(model, gpus=4, cpu_merge=True, cpu_relocation=False)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "class IntervalEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "        \n",
    "        self.auc_list = []\n",
    "        self.epoch_list = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            tf.contrib.summary.scalar('auc', tensor=score, step=epoch)\n",
    "            #print(\"\\n interval evaluation - epoch: {:d} - score: {:.6f}\".format(epoch, score))\n",
    "            \n",
    "            fpr_keras, tpr_keras, thresholds_keras = roc_curve(np.argmax(self.y_val, axis=1), \n",
    "                                                               y_pred[:, 1])\n",
    "            #print(fpr_keras, tpr_keras)\n",
    "            \n",
    "            auc_keras = auc(fpr_keras, tpr_keras)\n",
    "            print(auc_keras)\n",
    "            \n",
    "            plt.figure(int(epoch/10))\n",
    "            \n",
    "            plt.plot(fpr_keras, tpr_keras, label= str(epoch) + ' (area = {:.3f})'.format(auc_keras))\n",
    "            plt.plot([0, 1], [0, 1], 'k--')\n",
    "            plt.xlabel('False positive rate')\n",
    "            plt.ylabel('True positive rate')\n",
    "            plt.title('ROC curve')\n",
    "            plt.legend(loc='best')\n",
    "            #plt.show()\n",
    "            plt.savefig('./outputs/val_nb_readmit_auc_randomval' + str(epoch) + '.png')\n",
    "            \n",
    "            self.auc_list.append(score)\n",
    "            self.epoch_list.append(epoch)\n",
    "            plt.figure()\n",
    "            plt.plot(self.epoch_list, self.auc_list)\n",
    "            plt.savefig('./outputs/val_nb_readmit_auc_track_randomval.png')\n",
    "\n",
    "#ival = IntervalEvaluation(validation_data=(sess.run(val_ds.batch(10000).make_one_shot_iterator().get_next())), \n",
    "#                          interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelpath = './models/READMIT_val_best_randomval.hdf5'\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=100), \n",
    "#             ival, \n",
    "#              tb,\n",
    "             ModelCheckpoint(modelpath, monitor='val_loss', verbose=0, \n",
    "                             save_best_only=True, save_weights_only=False, mode='auto', period=1),\n",
    "            ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(train_ds.batch(128).prefetch(buffer_size=1), \n",
    "          steps_per_epoch=1000, epochs=1000,\n",
    "          validation_data=val_ds.batch(1000), validation_steps=10,\n",
    "          callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(modelpath)\n",
    "sample = sess.run(test_ds.batch(10000000).make_one_shot_iterator().get_next())\n",
    "y_pred_keras = model.predict(sample[0])\n",
    "\n",
    "score = roc_auc_score(sample[1], y_pred_keras)\n",
    "\n",
    "fpr_keras, tpr_keras, thresholds_keras = roc_curve(sample[1][:,1], y_pred_keras[:,1])\n",
    "\n",
    "#print(fpr_keras, tpr_keras)\n",
    "\n",
    "auc_keras = auc(fpr_keras, tpr_keras)\n",
    "print(auc_keras)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(fpr_keras, tpr_keras, label='Keras (area = {:.3f})'.format(auc_keras))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
